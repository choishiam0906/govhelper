# ì¤‘ì†Œë²¤ì²˜24 API + í¬ë¡¤ë§ ìŠ¤í¬ë¦½íŠ¸
# 2025ë…„ ì§€ì›ì‚¬ì—… ìˆ˜ì§‘ê¸°

import requests
import pandas as pd
from bs4 import BeautifulSoup
import time
import json
import re
from datetime import datetime
from urllib.parse import urljoin
import warnings
warnings.filterwarnings('ignore')

# ì¤‘ì†Œë²¤ì²˜24 API í‚¤
SMES_API_TOKEN = "/aK6H3+7h3rnbrBEB/rA3Af4qrDSNLhrrh0w6vDpt+g02fwpPpQK1Ms2AUceJNLu"

class SMES24APIClient:
    """ì¤‘ì†Œë²¤ì²˜24 API í´ë¼ì´ì–¸íŠ¸"""

    def __init__(self, token=SMES_API_TOKEN):
        self.token = token
        self.base_url = "https://www.smes.go.kr/fnct/apiReqst"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'application/json, text/plain, */*',
            'Accept-Language': 'ko-KR,ko;q=0.9',
            'Content-Type': 'application/x-www-form-urlencoded'
        })

    def get_announcements(self, page=1, size=100):
        """ê³µê³  ì •ë³´ ì¡°íšŒ API"""
        url = f"{self.base_url}/extPblancInfo"

        # GET ë°©ì‹ ì‹œë„
        params = {
            'token': self.token,
            'pageNo': page,
            'numOfRows': size
        }

        try:
            print(f"ğŸ”„ API í˜¸ì¶œ: {url}")
            response = self.session.get(url, params=params, timeout=30)

            if response.status_code == 200:
                data = response.json()
                if data.get('resultCd') == '00':
                    print(f"âœ… API ì„±ê³µ: {len(data.get('data', []))}ê°œ í•­ëª©")
                    return data.get('data', [])
                else:
                    print(f"âš ï¸ API ì‘ë‹µ: {data.get('resultMsg', 'Unknown error')}")

            # POST ë°©ì‹ ì‹œë„
            print("ğŸ”„ POST ë°©ì‹ìœ¼ë¡œ ì¬ì‹œë„...")
            response = self.session.post(url, data=params, timeout=30)

            if response.status_code == 200:
                data = response.json()
                if data.get('resultCd') == '00':
                    print(f"âœ… API ì„±ê³µ: {len(data.get('data', []))}ê°œ í•­ëª©")
                    return data.get('data', [])
                else:
                    print(f"âš ï¸ API ì‘ë‹µ: {data.get('resultMsg', 'Unknown error')}")

        except Exception as e:
            print(f"âŒ API ì˜¤ë¥˜: {e}")

        return []

    def get_all_announcements(self, max_pages=10):
        """ëª¨ë“  ê³µê³  ì •ë³´ ìˆ˜ì§‘"""
        all_data = []

        for page in range(1, max_pages + 1):
            print(f"ğŸ“„ í˜ì´ì§€ {page} ì¡°íšŒ ì¤‘...")
            data = self.get_announcements(page=page)

            if not data:
                break

            all_data.extend(data)
            time.sleep(1)  # API ë¶€í•˜ ë°©ì§€

        return all_data


class DirectSMESCrawler:
    """ì§ì ‘ ì›¹ í¬ë¡¤ë§ í´ë¼ì´ì–¸íŠ¸"""

    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
        })

        self.target_sites = {
            'mss': {
                'name': 'ì¤‘ì†Œë²¤ì²˜ê¸°ì—…ë¶€',
                'base_url': 'https://www.mss.go.kr',
                'search_urls': [
                    'https://www.mss.go.kr/site/smba/ex/bbs/List.do?cbIdx=310'
                ]
            },
            'bizinfo': {
                'name': 'ê¸°ì—…ë§ˆë‹¹',
                'base_url': 'https://www.bizinfo.go.kr',
                'search_urls': [
                    'https://www.bizinfo.go.kr/see/seea/selectSEEA120List.do'
                ]
            }
        }

        self.all_programs = []

    def get_page_content(self, url, params=None, retries=3):
        """í˜ì´ì§€ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°"""
        for attempt in range(retries):
            try:
                response = self.session.get(url, params=params, timeout=15)
                response.raise_for_status()
                response.encoding = 'utf-8'
                return response
            except Exception as e:
                if attempt < retries - 1:
                    time.sleep(2)
                    continue
                print(f"âŒ ìš”ì²­ ì‹¤íŒ¨: {url} - {e}")
                return None

    def is_2025_related(self, text):
        """2025ë…„ ê´€ë ¨ ì—¬ë¶€ í™•ì¸"""
        return '2025' in text

    def extract_organization(self, text):
        """ê¸°ê´€ëª… ì¶”ì¶œ"""
        patterns = [
            r'(ì¤‘ì†Œë²¤ì²˜ê¸°ì—…ë¶€|ì¤‘ì†Œë²¤ì²˜ê¸°ì—…ì§„í¥ê³µë‹¨|ì°½ì—…ì§„í¥ì›|ê¸°ìˆ ë³´ì¦ê¸°ê¸ˆ)',
            r'([ê°€-í£]+ì§„í¥ì›|[ê°€-í£]+ê³µë‹¨|[ê°€-í£]+í˜‘íšŒ)',
        ]
        for pattern in patterns:
            match = re.search(pattern, text)
            if match:
                return match.group(1).strip()
        return ""

    def crawl_mss(self):
        """ì¤‘ì†Œë²¤ì²˜ê¸°ì—…ë¶€ í¬ë¡¤ë§"""
        programs = []
        url = 'https://www.mss.go.kr/site/smba/ex/bbs/List.do?cbIdx=310'

        print(f"ğŸ¢ ì¤‘ì†Œë²¤ì²˜ê¸°ì—…ë¶€ í¬ë¡¤ë§...")
        response = self.get_page_content(url)

        if not response:
            return programs

        soup = BeautifulSoup(response.content, 'html.parser')
        rows = soup.select('tbody tr')

        for row in rows:
            try:
                title_elem = row.select_one('td.subject a, td a')
                if not title_elem:
                    continue

                title = title_elem.get_text(strip=True)
                content = row.get_text(separator=' ', strip=True)

                if self.is_2025_related(content):
                    href = title_elem.get('href', '')
                    link = urljoin('https://www.mss.go.kr', href) if href else ''

                    programs.append({
                        'title': title,
                        'content': content,
                        'link': link,
                        'organization': self.extract_organization(content) or 'ì¤‘ì†Œë²¤ì²˜ê¸°ì—…ë¶€',
                        'source': 'mss.go.kr',
                        'scraped_at': datetime.now().isoformat()
                    })
            except Exception as e:
                continue

        print(f"âœ… ì¤‘ì†Œë²¤ì²˜ê¸°ì—…ë¶€: {len(programs)}ê°œ ìˆ˜ì§‘")
        return programs

    def crawl_bizinfo(self):
        """ê¸°ì—…ë§ˆë‹¹ í¬ë¡¤ë§"""
        programs = []
        url = 'https://www.bizinfo.go.kr/see/seea/selectSEEA120List.do'

        print(f"ğŸ¢ ê¸°ì—…ë§ˆë‹¹ í¬ë¡¤ë§...")

        # POST ìš”ì²­ìœ¼ë¡œ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        data = {
            'pageIndex': 1,
            'recordCountPerPage': 100,
            'pblancSe': '',  # ì „ì²´
            'bizPldirCode': '',
            'bsnsSportSe': '',
            'areaNm': '',
            'jrsdInsttNm': '',
            'searchKwrd': '2025'  # 2025ë…„ ê²€ìƒ‰
        }

        try:
            response = self.session.post(url, data=data, timeout=30)
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                items = soup.select('.tbl_list tbody tr, .list_item, .biz_list li')

                for item in items:
                    title_elem = item.select_one('a, .title, .subject')
                    if title_elem:
                        title = title_elem.get_text(strip=True)
                        content = item.get_text(separator=' ', strip=True)

                        if self.is_2025_related(content) and title:
                            programs.append({
                                'title': title,
                                'content': content,
                                'link': '',
                                'organization': self.extract_organization(content) or 'ê¸°ì—…ë§ˆë‹¹',
                                'source': 'bizinfo.go.kr',
                                'scraped_at': datetime.now().isoformat()
                            })
        except Exception as e:
            print(f"âŒ ê¸°ì—…ë§ˆë‹¹ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")

        print(f"âœ… ê¸°ì—…ë§ˆë‹¹: {len(programs)}ê°œ ìˆ˜ì§‘")
        return programs

    def crawl_all(self):
        """ëª¨ë“  ì‚¬ì´íŠ¸ í¬ë¡¤ë§"""
        print("ğŸš€ ì›¹ í¬ë¡¤ë§ ì‹œì‘...")

        self.all_programs.extend(self.crawl_mss())
        time.sleep(2)
        self.all_programs.extend(self.crawl_bizinfo())

        # ì¤‘ë³µ ì œê±°
        seen = set()
        unique = []
        for p in self.all_programs:
            key = p['title']
            if key not in seen:
                seen.add(key)
                unique.append(p)

        self.all_programs = unique
        print(f"ğŸ“Š ì´ {len(self.all_programs)}ê°œ í”„ë¡œê·¸ë¨ ìˆ˜ì§‘ ì™„ë£Œ")
        return self.all_programs


def save_to_excel(df, filename='ì¤‘ì†Œë²¤ì²˜24_ì§€ì›ì‚¬ì—…_2025.xlsx'):
    """Excel ì €ì¥ (pandas ìµœì‹  ë²„ì „ í˜¸í™˜)"""
    if df is None or df.empty:
        print("ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return False

    try:
        # pandas ìµœì‹  ë²„ì „: encoding íŒŒë¼ë¯¸í„° ì œê±°
        df.to_excel(filename, index=False, engine='openpyxl')
        print(f"âœ… ì €ì¥ ì™„ë£Œ: {filename}")
        return True
    except Exception as e:
        print(f"âŒ ì €ì¥ ì‹¤íŒ¨: {e}")
        # CSVë¡œ ëŒ€ì²´ ì €ì¥
        csv_filename = filename.replace('.xlsx', '.csv')
        df.to_csv(csv_filename, index=False, encoding='utf-8-sig')
        print(f"ğŸ“ CSVë¡œ ì €ì¥: {csv_filename}")
        return True


def save_to_json(data, filename='ì¤‘ì†Œë²¤ì²˜24_ì§€ì›ì‚¬ì—…_2025.json'):
    """JSON ì €ì¥"""
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"âœ… JSON ì €ì¥: {filename}")
        return True
    except Exception as e:
        print(f"âŒ JSON ì €ì¥ ì‹¤íŒ¨: {e}")
        return False


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    print("=" * 60)
    print("ğŸš€ ì¤‘ì†Œë²¤ì²˜24 ì§€ì›ì‚¬ì—… ìˆ˜ì§‘ê¸°")
    print("=" * 60)

    all_data = []

    # 1. API ì‹œë„
    print("\nğŸ“¡ [1ë‹¨ê³„] API ì—°ë™ ì‹œë„...")
    api_client = SMES24APIClient()
    api_data = api_client.get_all_announcements(max_pages=5)

    if api_data:
        print(f"âœ… APIì—ì„œ {len(api_data)}ê°œ ìˆ˜ì§‘")
        all_data.extend(api_data)
    else:
        print("âš ï¸ API ì—°ë™ ì‹¤íŒ¨ - í¬ë¡¤ë§ìœ¼ë¡œ ì „í™˜")

    # 2. ì›¹ í¬ë¡¤ë§
    print("\nğŸ•·ï¸ [2ë‹¨ê³„] ì›¹ í¬ë¡¤ë§...")
    crawler = DirectSMESCrawler()
    crawl_data = crawler.crawl_all()

    if crawl_data:
        all_data.extend(crawl_data)

    # 3. ê²°ê³¼ ì •ë¦¬
    if all_data:
        df = pd.DataFrame(all_data)

        # ì¤‘ë³µ ì œê±°
        if 'title' in df.columns:
            df = df.drop_duplicates(subset=['title'])

        print(f"\nğŸ“Š ìµœì¢… ê²°ê³¼: {len(df)}ê°œ ì§€ì›ì‚¬ì—…")

        # ì €ì¥
        save_to_excel(df)
        save_to_json(all_data)

        # ë¯¸ë¦¬ë³´ê¸°
        print("\nğŸ“‹ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°:")
        print(df[['title', 'organization', 'source']].head(5).to_string() if 'source' in df.columns else df.head(5).to_string())

        return df
    else:
        print("âŒ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None


if __name__ == "__main__":
    df = main()
